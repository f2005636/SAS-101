Customer Level Behavior Scorecard 
Objective and Data Sources:

We have various types of scorecards like acquisition, behavior, income, collection etc. 
The purpose of building a behavior scorecard is to monitor the performance of booked accounts, i.e. accounts which are already in Citi’s books. We use the scorecard to see the performance and transition of accounts across various delinquency buckets and how they are performing over a given time horizon (known as the performance window – derived from vintage analysis - during model development process and we also monitor them during the post-performance period as well as the Out-Of-Time period to check the performance of our model).

The prime objective of this model is to generate behavior scores which would be used for portfolio decision in conjunction with existing application score used for the purpose of collection/portfolio review. Only internal bank data is used, development data uses 4 vintages (Jan 15, Apr 15, Jul 15 and Oct 15) and out of time data uses 3 vintages (Jan 16, Apr 16, Jul 16). 
•	Data aggregation for DEV 25% random sample from each one of the 4 vintages
•	Data aggregation for OOT Validation 33.3% random sample from each one of the 3 vintages

Modeling Methodology:
1.	Data 
2.	Data preprocessing 
3.	Development sample 
4.	Variable reduction 
a.	Basic reduction 
b.	Information value
c.	Correlation to target 
5.	Weight of Evidence binning 
a.	Fine classing 
b.	Coarse classing 
c.	Beta = 1 for good binning 
6.	Model fitting 
a.	Multi-collinearity checks
b.	Removing insignificant variables 
c.	Stepwise selection 
7.	Model diagnostics 
a.	P value 
b.	Lift / KS
c.	Gini / ROC
8.	Model validation 
a.	Scaling 
b.	PSI
c.	Bin re-engineering 
9.	Final model 

Variable reduction process:
There are various variable reduction processes which are followed during a model development process. They are as follows:
	Removing variables with std dev = 0
	Removing variables with a high missing %
	Information value check (variables with IV<0.1 and IV>0.5 can be removed as weak and over predicting respectively)
	Bi- variate Trend Analysis ( checking the trend of the variable with the target variable -  should follow business intuition – a bubble plot can be used for this purpose)
	Proc Varclus can be used for Multi variate variable reduction. Choosing the top 3-4 variables which have a low 1-R^2 ratio and high IV is a good technique to follow.
	Multicollinearity removal – variables with VIF > 2 should be removed due to multicollinearity.

•	Starting = 300
•	Non predictive var removal: keys, ids, etc = 10
•	Basic reduction: 0 std, 100% missing = 90
•	Low IV reduction = 100
•	Low correlation to target reduction = 50

•	Variable input to logistic regression = 50
•	Reduction to satisfy significance, multicollinearity = 42
•	Final variables in scorecard = 8

Final variables in the model
1.	Count of delinquency > 0 in last 6 cycles  (A)
2.	Number of cash advances in last 6 cycles (B)
3.	Current balance as a % of max balance in last 6 cycles (C)
4.	Payment ratio (D)
5.	Amount of over limit in last 6 cycles (E)
6.	Number of cycles since highest cash use amount (F) 
7.	Number of purchases in last 6 cycles (G) 
8.	Outstanding balance in last 6 cycles (H)

Parameter 	DF	Estimate 	Standard Error 	Wald Chi Square 	Pr > Chi Square	Marginal KS	VIF
Intercept 	1	-3.886	0.0106	134542	<0.0001		
A	1	0.714	0.0106	4573	<0.0001	3.1	1.12
B	1	0.155	0.0142	118	<0.0001	0.1	1.54
C	1	0.627	0.0125	2515	<0.0001	3.6	1.43
D	1	0.323	0.0128	634	<0.0001	1.3	1.45
E	1	0.292	0.0121	585	<0.0001	0.2	1.14
F	1	0.448	0.0169	703	<0.0001	0.6	1.27
G	1	0.490	0.0283	299	<0.0001	0.3	1.54
H	1	0.193	0.0153	158	<0.0001	0.2	1.31

Best Practice:
•	Statistical significance: <0.0001
•	VIF < 2
•	Marginal KS > 0

Model performance

1.	Concordance
Model performance can be checked with the concordance value. 

	A pair is concordant if 1 (observation with the desired outcome i.e. event) has a higher predicted probability than 0 (observation without the outcome i.e. non-event).
	A pair is discordant if 0 (observation without the desired outcome i.e. non-event) has a higher predicted probability than 1 (observation with the outcome i.e. event).
	A pair is tied if 1 (observation with the desired outcome i.e. event) has same predicted probability than 0 (observation without the outcome i.e. non-event).

The final percent values are calculated using the formula below –

Percent Concordant = (Number of concordant pairs)/Total number of pairs

Percent Discordance = (Number of discordant pairs)/Total number of pairs

Percent Tied = (Number of tied pairs)/Total number of pairs

a.	%concordant = 85.7%
b.	%discordant = 14.2%
c.	%tied = 0.1%


2.	AUROC
AUROC gives the area under the ROC curve. It is plotted as a graph between sensitivity and 1-specificity, which we can get from the confusion matrix. An ideal model will have AUROC very close to 1. 

Sensitivity is the ability of a model to correctly predict y=1 and specificity is the ability of a model to correctly predict y=0.
Sensitivity is calculated as True Positive/ (True Positive + False Negative) while specificity is calculated as True Negative/ (False Positive + True Negative).

                
	Actual
Model		Good	Bad
	Good	True Positive	False Positive
	Bad	False Negative	True Negative
a.	0.858


3.	KS
The KS statistic gives us the separation power of the model. It is calculated as the maximum of the absolute value of the difference between cumulative non – event and cumulative event. A good model will have a KS > 30. A high value of KS (eg. KS = 80 ) will depict over – prediction in the model. 

We have two important concepts of Gains and Lifts which we can get from the KS table.
	Gain
Gain at a given decile level is the ratio of cumulative number of targets (events) up to that decile to the total number of targets (events) in the entire data set.
	Interpretation: 
% of targets (events) covered at a given decile level. For example, 80% of targets covered in top 20% of data based on model. We can say we can identify and target 80% of customers who are likely to be bad by just analyzing 20% of total customers.

	Lift
It measures how much better one can expect to do with the predictive model comparing without a model. It is the ratio of gain % to the random expectation % at a given decile level. The random expectation at the xth decile is x%.
	Interpretation: 
	The Cum Lift of 4.03 for top two deciles means that when selecting 20% of the records based on the model, one can expect 4.03 times the total number of targets (events) found by randomly selecting 20%-of-file without a model.
a.	57.07

 


